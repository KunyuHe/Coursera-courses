1. In a Bayesian simple linear regression y¡«N(¦Á+x¦Â,¦Ò2)
Suppose our priors on the parameters ¦Á,¦Â,¦Ò2 are independent and that the prior on ¦Â is N(0,1). How will the posterior mean of ¦Â compare to least squares estimate of ¦Â?
A. The mean of the posterior distribution of ¦Â will be closer to zero than the least squares estimate. +

1.1 True or False: The mean and standard deviation of the posterior distribution of a slope or intercept parameter in Bayesian linear regression is equal to the least squares estimate and corresponding standard error if the reference prior is used and normally distributed errors are assumed.
A. TRUE +

2. A linear model was estimated using Bayesian methods to predict the height of a male based on his age. All males used in the data are between the ages of 3 to 9 years old. Is it appropriate to use this model to predict the height of a 21 year old man?
A. No, since extrapolating outside the range of age observed in the data set may result in a nonsensical prediction. +

2.1 A simple linear model (either Bayesian or frequentist) that tries to predict an individual's height from his/her age is unlikely to perform well, since human growth rates are non-linear with regard to age. Specifically, humans tend to grow quickly early in life, stop growing at through most of adulthood, and sometimes shrink somewhat when they get old. Which of the following modifications to a simple linear regression model should you prefer?
A. Log-transforming the dependent variable (height) to account for skewness. X
B. Including other relevant covariates such as weight or income. X
C. Imposing strong prior distributions on the parameters in a Bayesian analysis.

3. Suppose we want to set a level k such that if we observe a data point more than k standard deviations away from the mean, we deem it an outlier. If the number of observations is 1000, what is the probability that we observe an outlier at least 4 standard deviations away from its prediction value?
A. 0.06 +

3.1 You fit a linear model on 1000 data points and identify a point that lies 3 standard deviations above its predicted value. Should you worry about this potential outlier? Why or why not?
A. No, because the probability that all 1000 points will be within 3 standard deviations of their predicted values is 0.07, so it is unsurprising to observe a point 3 standard deviations away from its predicted value. +

4. Suppose we use Bayesian methods (with a prior distribution) to fit a linear model in order to predict the final sale price of a home based on quantifiable attributes of the home. If the 95% posterior predictive interval of a new home (not in the data set) is (312,096, 392,097), which of the following statements represents a correct interpretation of this interval?
A. The probability that the house will sell for between 312,096 and 392,097 is 0.95. +

4.1 Suppose a researcher is using Bayesian multiple regression to quantify the effect of vitamin C on cancer patient mortality. The central 95% posterior credible interval of the coefficient of vitamin C dosage is (-0.19, -0.07). Assuming the model assumptions are valid, what can we say about the effect of vitamin C on cancer patient mortality?
A. The posterior probability that the coefficient of vitamin C is greater than zero is low, so there is a high posterior probability of a negative association between vitamin C and cancer patient mortality. +

5. Which of the following is not a principled way to select a model?
A. Pick the model with the highest R2 +

5.1 Which of the following goes into the calculation of the Bayesian Information Criterion (BIC)?
A. The maximum value of the log-likelihood under the current model, the sample size, and the number of parameters in the model. +

6. In a linear model with an intercept term (that is always included) and 3 potential predictors, how many possible models are there?
A. 8 +

7. Can Bayesian model averaging be done with a large amount of predictors?
A. Yes, but Monte Carlo sampling techniques will need to be done to approximate the posterior distribution + 

7.1 Suppose that a MCMC sampler is currently visiting model B. Model A has a higher posterior probability than model B and Model C has a lower posterior probability than model B. Which of the following statements is true in the MCMC algorithm?
A. If a jump to Model C is proposed, this jump is always accepted. X

8. Which of the following is not a useful method of checking a linear model after it is fit?
A. Ensuring that R2 is as close to 1 as possible. +

8.1 Which of the following is not an assumption made in Bayesian multiple regression?
A. The errors have positive autocorrelation. +

9. Which of the following is an advantage of using the Zellner-Siow-Cauchy prior in Bayesian model averaging?
A. It allows for uncertainty in the prior variance parameter g. +
B. It prevents BMA from disproportionately favoring the null model as a result of the Bartlett-Lindley paradox. +

9.1 Why is the Zellner g-prior useful in Bayesian model averaging?
A. It simplifies prior elicitation down to two components, the prior mean and g. +

10. When selecting a single model from an ensemble of models in the case of Bayesian model averaging, which of the following selection procedures corresponds to choosing the "highest probability model"?
A. Selecting the model with the highest posterior model probability. + 

10.1 When selecting a single model from an ensemble of models in the case of Bayesian model averaging, which of the following selection procedures corresponds to choosing the "median probability model"?
A. Including only the coefficients with posterior model inclusion probability above 0.5. +